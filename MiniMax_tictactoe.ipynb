{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from games.tictactoe import TicTacToe\n",
    "from agents.agent_random import RandomAgent\n",
    "from agents.minimax import MiniMax\n",
    "from base.utils import play, run\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = TicTacToe(render_mode='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({-1: 35, 0: 12, 1: 53}, 0.18)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents = dict(map(lambda agent: (agent, RandomAgent(game=game, agent=agent)), game.agents))\n",
    "run(game=game, agents=agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'raw_env' object has no attribute '_cumulative_rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4932\\1815940035.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMiniMax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\base\\utils.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(game, agents, N)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\agents\\minimax.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\agents\\minimax.py\u001b[0m in \u001b[0;36mminimax\u001b[1;34m(self, game, depth)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mchild\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m             \u001b[0maction_nodes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\games\\tictactoe.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cumulative_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\anaconda3\\lib\\site-packages\\pettingzoo\\classic\\tictactoe\\tictactoe.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;31m# Switch selection to next agents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cumulative_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_selection\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent_selection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_agent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'raw_env' object has no attribute '_cumulative_rewards'"
     ]
    }
   ],
   "source": [
    "agents[game.agents[0]] = MiniMax(game=game, agent=game.agents[0], depth=2)\n",
    "run(game=game, agents=agents, N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:<class 'numpy.ndarray'>\n",
      "DEBUG:root:[[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n",
      "DEBUG:root:(0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "DEBUG:root:Policy: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "DEBUG:root:Legal moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "DEBUG:root:Legal move probabilities: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "DEBUG:root:Chosen action: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'raw_env' object has no attribute '_cumulative_rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\MiniMax_tictactoe.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maria/OneDrive/Documents/GitHub/AlternatingGames/MiniMax_tictactoe.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agents[game\u001b[39m.\u001b[39magents[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m MiniMax(game\u001b[39m=\u001b[39mgame, agent\u001b[39m=\u001b[39mgame\u001b[39m.\u001b[39magents[\u001b[39m1\u001b[39m], depth\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maria/OneDrive/Documents/GitHub/AlternatingGames/MiniMax_tictactoe.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m run(game\u001b[39m=\u001b[39;49mgame, agents\u001b[39m=\u001b[39;49magents, N\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\base\\utils.py:19\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(game, agents, N)\u001b[0m\n\u001b[0;32m     17\u001b[0m game\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     18\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m game\u001b[39m.\u001b[39mterminated():\n\u001b[1;32m---> 19\u001b[0m     action \u001b[39m=\u001b[39m agents[game\u001b[39m.\u001b[39;49magent_selection]\u001b[39m.\u001b[39;49maction()\n\u001b[0;32m     20\u001b[0m     game\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     21\u001b[0m values\u001b[39m.\u001b[39mappend(game\u001b[39m.\u001b[39mreward(game\u001b[39m.\u001b[39magents[\u001b[39m0\u001b[39m]))\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\agents\\minimax.py:13\u001b[0m, in \u001b[0;36mMiniMax.action\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maction\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 13\u001b[0m     act, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgame, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdepth)\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m act\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\agents\\minimax.py:36\u001b[0m, in \u001b[0;36mMiniMax.minimax\u001b[1;34m(self, game, depth)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m actions:\n\u001b[0;32m     35\u001b[0m     child \u001b[39m=\u001b[39m game\u001b[39m.\u001b[39mclone()\n\u001b[1;32m---> 36\u001b[0m     child\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     37\u001b[0m     action_nodes\u001b[39m.\u001b[39mappend((action, child))\n\u001b[0;32m     39\u001b[0m \u001b[39mif\u001b[39;00m agent \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent: \u001b[39m# Min\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\games\\tictactoe.py:48\u001b[0m, in \u001b[0;36mTicTacToe.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m---> 48\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     49\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update()\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\pettingzoo\\classic\\tictactoe\\tictactoe.py:231\u001b[0m, in \u001b[0;36mraw_env.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mterminations \u001b[39m=\u001b[39m {i: \u001b[39mTrue\u001b[39;00m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magents}\n\u001b[0;32m    230\u001b[0m \u001b[39m# Switch selection to next agents\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cumulative_rewards[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m    232\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent_selection \u001b[39m=\u001b[39m next_agent\n\u001b[0;32m    234\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accumulate_rewards()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'raw_env' object has no attribute '_cumulative_rewards'"
     ]
    }
   ],
   "source": [
    "agents[game.agents[1]] = MiniMax(game=game, agent=game.agents[1], depth=2)\n",
    "run(game=game, agents=agents, N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from importlib import reload\n",
    "import agents\n",
    "reload(agents)\n",
    "reload(agents.counterfactualregret)\n",
    "import games\n",
    "from games.tictactoe import TicTacToe\n",
    "from agents.agent_random import RandomAgent\n",
    "from agents.minimax import MiniMax\n",
    "from base.utils import play, run\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(agents.counterfactualregret)\n",
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "reload(games.tictactoe)\n",
    "from games.tictactoe import TicTacToe\n",
    "\n",
    "game = TicTacToe(render_mode='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'player_1': 0.0, 'player_2': 0.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.observe('player_1')\n",
    "game._cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-01 20:42:26,796 - DEBUG - Training agent player_1\n",
      "2023-11-01 20:42:26,802 - DEBUG - Node agent: player_1\n",
      "2023-11-01 20:42:26,803 - DEBUG - Legal moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "2023-11-01 20:42:26,805 - DEBUG - Attempting action 0 in game state (0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "2023-11-01 20:42:26,806 - DEBUG - <class 'str'>\n",
      "2023-11-01 20:42:26,807 - DEBUG - (2,)\n",
      "2023-11-01 20:42:26,809 - DEBUG - 0.1111111111111111\n",
      "2023-11-01 20:42:26,809 - DEBUG - 2\n",
      "2023-11-01 20:42:26,811 - DEBUG - Current game state: TicTacToe\n",
      "2023-11-01 20:42:26,812 - DEBUG - Action: 0\n",
      "2023-11-01 20:42:26,814 - DEBUG - Legal moves: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'raw_env' object has no attribute '_cumulative_rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4932\\3562201009.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# You can adjust the number of iterations\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\agents\\counterfactualregret.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, niter)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# logging.debug(f\"Training agent {self.agent}\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mniter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcfr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcfr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\agents\\counterfactualregret.py\u001b[0m in \u001b[0;36mcfr\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[0mprobability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_agents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcfr_rec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcfr_rec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAlternatingGame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAgentID\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\agents\\counterfactualregret.py\u001b[0m in \u001b[0;36mcfr_rec\u001b[1;34m(self, game, agent, probability)\u001b[0m\n\u001b[0;32m    137\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Action: {a}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Legal moves: {game.available_actions()}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m             \u001b[1;31m# call cfr recursively on updated game with new probability and update node utility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\OneDrive\\Master en Big Data - Universidad ORT\\3er Semestre\\Sistemas Multiagente\\AlternatingGames\\games\\tictactoe.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0magent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cumulative_rewards\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\scruple077\\anaconda3\\lib\\site-packages\\pettingzoo\\classic\\tictactoe\\tictactoe.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[1;32mdef\u001b[0m \u001b[0mgetSymbol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"-\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'raw_env' object has no attribute '_cumulative_rewards'"
     ]
    }
   ],
   "source": [
    "agents = dict(map(lambda agent: (agent, CounterFactualRegret(game=game, agent=agent)), game.agents))\n",
    "\n",
    "for agent in agents.values():\n",
    "    agent.train(niter=100)  # You can adjust the number of iterations\n",
    "    \n",
    "run(game=game, agents=agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 0, 0],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2039950914416 2039924056912\n"
     ]
    }
   ],
   "source": [
    "game1 = TicTacToe(render_mode='')\n",
    "game2 = game.clone()\n",
    "print(id(game1), id(game2))  # Should print different IDs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.counterfactualregret import CounterFactualRegret\n",
    "from importlib import reload\n",
    "import agents\n",
    "reload(agents)\n",
    "reload(agents.counterfactualregret)\n",
    "\n",
    "from games.kuhn import KuhnPoker\n",
    "from agents.agent_random import RandomAgent\n",
    "from base.utils import play, run\n",
    "import numpy as np\n",
    "\n",
    "game = KuhnPoker(render_mode='human')\n",
    "reload(agents.counterfactualregret)\n",
    "from agents.counterfactualregret import CounterFactualRegret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-29 19:28:17,284 - DEBUG - Training agent agent_0\n",
      "2023-10-29 19:28:17,286 - DEBUG - Node agent: agent_0\n",
      "2023-10-29 19:28:17,287 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,288 - DEBUG - Attempting action 0 in game state 1\n",
      "2023-10-29 19:28:17,289 - DEBUG - <class 'str'>\n",
      "2023-10-29 19:28:17,290 - DEBUG - (2,)\n",
      "2023-10-29 19:28:17,291 - DEBUG - 0.5\n",
      "2023-10-29 19:28:17,291 - DEBUG - 2\n",
      "2023-10-29 19:28:17,293 - DEBUG - Current game state: KuhnPoker\n",
      "2023-10-29 19:28:17,294 - DEBUG - Action: 0\n",
      "2023-10-29 19:28:17,294 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,295 - DEBUG - Before recursive call, game state: KuhnPoker, attempting action: 0\n",
      "2023-10-29 19:28:17,296 - DEBUG - Node agent: agent_1\n",
      "2023-10-29 19:28:17,297 - DEBUG - Entering eval_node, game state: 2p\n",
      "2023-10-29 19:28:17,298 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,299 - DEBUG - Attempting action 0 in game state 2p\n",
      "2023-10-29 19:28:17,300 - DEBUG - Node agent: agent_0\n",
      "2023-10-29 19:28:17,301 - DEBUG - Before undo, game state: 2pp\n",
      "2023-10-29 19:28:17,302 - DEBUG - After undo, game state: 2pp\n",
      "2023-10-29 19:28:17,303 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,303 - DEBUG - Attempting action 1 in game state 2pp\n",
      "Game has already finished - Call reset if you want to play again\n",
      "2023-10-29 19:28:17,305 - DEBUG - Node agent: agent_0\n",
      "2023-10-29 19:28:17,306 - DEBUG - Before undo, game state: 2pp\n",
      "2023-10-29 19:28:17,306 - DEBUG - After undo, game state: 2pp\n",
      "2023-10-29 19:28:17,307 - DEBUG - Exiting eval_node, game state: KuhnPoker\n",
      "2023-10-29 19:28:17,308 - DEBUG - After recursive call, game state: 1p\n",
      "2023-10-29 19:28:17,308 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,309 - DEBUG - Attempting action 1 in game state 1p\n",
      "2023-10-29 19:28:17,310 - DEBUG - <class 'str'>\n",
      "2023-10-29 19:28:17,310 - DEBUG - (2,)\n",
      "2023-10-29 19:28:17,311 - DEBUG - 0.5\n",
      "2023-10-29 19:28:17,312 - DEBUG - 2\n",
      "2023-10-29 19:28:17,313 - DEBUG - Current game state: KuhnPoker\n",
      "2023-10-29 19:28:17,313 - DEBUG - Action: 1\n",
      "2023-10-29 19:28:17,314 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,315 - DEBUG - Before recursive call, game state: KuhnPoker, attempting action: 1\n",
      "2023-10-29 19:28:17,316 - DEBUG - Node agent: agent_0\n",
      "2023-10-29 19:28:17,317 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,318 - DEBUG - Attempting action 0 in game state 1pb\n",
      "2023-10-29 19:28:17,319 - DEBUG - <class 'str'>\n",
      "2023-10-29 19:28:17,319 - DEBUG - (2,)\n",
      "2023-10-29 19:28:17,320 - DEBUG - 0.5\n",
      "2023-10-29 19:28:17,321 - DEBUG - 2\n",
      "2023-10-29 19:28:17,321 - DEBUG - Current game state: KuhnPoker\n",
      "2023-10-29 19:28:17,322 - DEBUG - Action: 0\n",
      "2023-10-29 19:28:17,323 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,324 - DEBUG - Before recursive call, game state: KuhnPoker, attempting action: 0\n",
      "2023-10-29 19:28:17,325 - DEBUG - Node agent: agent_1\n",
      "2023-10-29 19:28:17,326 - DEBUG - After recursive call, game state: 1pbp\n",
      "2023-10-29 19:28:17,327 - DEBUG - Legal moves: [0, 1]\n",
      "2023-10-29 19:28:17,328 - DEBUG - Attempting action 1 in game state 1pbp\n",
      "2023-10-29 19:28:17,328 - DEBUG - <class 'str'>\n",
      "2023-10-29 19:28:17,330 - DEBUG - (2,)\n",
      "2023-10-29 19:28:17,330 - DEBUG - 0.5\n",
      "2023-10-29 19:28:17,331 - DEBUG - 2\n",
      "2023-10-29 19:28:17,332 - DEBUG - Current game state: KuhnPoker\n",
      "2023-10-29 19:28:17,333 - DEBUG - Action: 1\n",
      "2023-10-29 19:28:17,334 - DEBUG - Legal moves: [0, 1]\n",
      "Game has already finished - Call reset if you want to play again\n",
      "2023-10-29 19:28:17,334 - DEBUG - Before recursive call, game state: KuhnPoker, attempting action: 1\n",
      "2023-10-29 19:28:17,335 - DEBUG - Node agent: agent_1\n",
      "2023-10-29 19:28:17,336 - DEBUG - After recursive call, game state: 1pbp\n",
      "2023-10-29 19:28:17,337 - DEBUG - Node utility: -2.0\n",
      "2023-10-29 19:28:17,338 - DEBUG - Utility: [-2. -2.]\n",
      "2023-10-29 19:28:17,339 - DEBUG - Probability: [0.5 1. ]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\MiniMax_tictactoe.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maria/OneDrive/Documents/GitHub/AlternatingGames/MiniMax_tictactoe.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agents \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m agent: (agent, CounterFactualRegret(game\u001b[39m=\u001b[39mgame, agent\u001b[39m=\u001b[39magent)), game\u001b[39m.\u001b[39magents))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maria/OneDrive/Documents/GitHub/AlternatingGames/MiniMax_tictactoe.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m agent \u001b[39min\u001b[39;00m agents\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maria/OneDrive/Documents/GitHub/AlternatingGames/MiniMax_tictactoe.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     agent\u001b[39m.\u001b[39;49mtrain(niter\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)  \u001b[39m# You can adjust the number of iterations\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maria/OneDrive/Documents/GitHub/AlternatingGames/MiniMax_tictactoe.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m run(game\u001b[39m=\u001b[39mgame, agents\u001b[39m=\u001b[39magents)\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\agents\\counterfactualregret.py:77\u001b[0m, in \u001b[0;36mCounterFactualRegret.train\u001b[1;34m(self, niter)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m# logging.debug(f\"Training agent {self.agent}\")\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(niter):\n\u001b[1;32m---> 77\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfr()\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\agents\\counterfactualregret.py:84\u001b[0m, in \u001b[0;36mCounterFactualRegret.cfr\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m game\u001b[39m.\u001b[39mreset()\n\u001b[0;32m     83\u001b[0m probability \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones(game\u001b[39m.\u001b[39mnum_agents)\n\u001b[1;32m---> 84\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfr_rec(game\u001b[39m=\u001b[39;49mgame, agent\u001b[39m=\u001b[39;49magent, probability\u001b[39m=\u001b[39;49mprobability)\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\agents\\counterfactualregret.py:144\u001b[0m, in \u001b[0;36mCounterFactualRegret.cfr_rec\u001b[1;34m(self, game, agent, probability)\u001b[0m\n\u001b[0;32m    142\u001b[0m node, obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_node(node_agent,game)\n\u001b[0;32m    143\u001b[0m logging\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBefore recursive call, game state: \u001b[39m\u001b[39m{\u001b[39;00mgame\u001b[39m}\u001b[39;00m\u001b[39m, attempting action: \u001b[39m\u001b[39m{\u001b[39;00ma\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m utility[a] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcfr_rec(game, agent, probability_new)\n\u001b[0;32m    145\u001b[0m node, obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_node(node_agent,game)        \n\u001b[0;32m    146\u001b[0m logging\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAfter recursive call, game state: \u001b[39m\u001b[39m{\u001b[39;00mobs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\agents\\counterfactualregret.py:159\u001b[0m, in \u001b[0;36mCounterFactualRegret.cfr_rec\u001b[1;34m(self, game, agent, probability)\u001b[0m\n\u001b[0;32m    156\u001b[0m     logging\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUtility: \u001b[39m\u001b[39m{\u001b[39;00mutility\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    157\u001b[0m     logging\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProbability: \u001b[39m\u001b[39m{\u001b[39;00mprobability\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 159\u001b[0m     node\u001b[39m.\u001b[39;49mupdate(utility\u001b[39m=\u001b[39;49mutility, node_utility\u001b[39m=\u001b[39;49mnode_utility, probability\u001b[39m=\u001b[39;49mprobability)\n\u001b[0;32m    160\u001b[0m     node\u001b[39m.\u001b[39mupdate_strategy()  \n\u001b[0;32m    162\u001b[0m \u001b[39mreturn\u001b[39;00m node_utility\n",
      "File \u001b[1;32mc:\\Users\\maria\\OneDrive\\Documents\\GitHub\\AlternatingGames\\agents\\counterfactualregret.py:23\u001b[0m, in \u001b[0;36mNode.update\u001b[1;34m(self, utility, node_utility, probability)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, utility, node_utility, probability) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 23\u001b[0m     action_regrets \u001b[39m=\u001b[39m (utility \u001b[39m-\u001b[39m node_utility) \u001b[39m/\u001b[39m probability[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent]\n\u001b[0;32m     24\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_regrets \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmaximum(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcumulative_regrets \u001b[39m+\u001b[39m action_regrets)\n",
      "\u001b[1;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "agents = dict(map(lambda agent: (agent, CounterFactualRegret(game=game, agent=agent)), game.agents))\n",
    "\n",
    "for agent in agents.values():\n",
    "    agent.train(niter=100)  # You can adjust the number of iterations\n",
    "    \n",
    "run(game=game, agents=agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.available_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2b'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()\n",
    "game.step(1)\n",
    "game.observe('agent_0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
